---
title: "Model_example"
author: "Konstantinos Patelis"
date: "24/04/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r libraries}

library(here)
library(tidyverse)
library(data.table)
library(dtplyr)
library(tidyfast)
library(tidytext)
library(lexicon)
library(knitr)
theme_set(theme_bw())

source(here("analysis", "functions.R"))
```


```{r load_data, warning=FALSE}

k <- .1

tweets_loc <- here("data", "final", "en_US", "en_US.twitter.txt")
news_loc <- here("data", "final", "en_US", "en_US.news.txt")
blogs_loc <- here("data", "final", "en_US", "en_US.blogs.txt")

tweets <- read_text(tweets_loc, prows = k, seed = 1234) %>%
  as_tibble() %>%
  mutate(source = "tweets", id = paste0("tweet_", row_number()))

news <- read_text(news_loc, prows = k, seed = 5678) %>%
  as_tibble() %>%
  mutate(source = "news", id = paste0("news_", row_number()))

blogs <- read_text(blogs_loc, prows = k, seed = 9012) %>%
  as_tibble() %>%
  mutate(source = "blogs", id = paste0("blog_", row_number()))

corpora <- bind_rows(tweets, news, blogs) %>%
  unnest_tokens(output = "text", input = "value", token = "sentences") %>%
  mutate(text = paste0("BOS ", text, " EOS"))

profanity <- tibble(
  word = c(
    profanity_alvarez,
    profanity_arr_bad,
    profanity_banned,
    profanity_racist,
    profanity_zac_anger)
  ) %>%
  distinct() %>% 
  unlist()



# text_split <- split_text(tweets_loc, news_loc, blogs_loc, names = c("tweets", "news", "blogs"), split = .8, prop = 1, seed = 1234)

# stop <- unlist(stop_words)

# train <- text_split$train %>%
#   unnest_tokens(output = "text", input = "value", token = "sentences") %>% 
#   mutate(text = paste0("BOS ", text, " EOS"))
# 
# test <- text_split$test %>%
#   unnest_tokens(output = "text", input = "value", token = "sentences") %>% 
#   mutate(text = paste0("BOS ", text, " EOS"))

```

```{r}

make_ngram <- function(data, n = 6, id = id, value = value, word_len_max = 18, top_word_freq = NULL, profanity_filter = NULL) {
  id_q <- deparse(substitute(id))
  value_q <- deparse(substitute(value))
  word_vec <- paste0("word_", 1:n)
  ngram_final <- NULL
  data_dt <- copy(as.data.table(data))

  stopifnot(
    "Please specify existing id name" = id_q %in% colnames(data_dt),
    "Please specify existing text column name" = value_q %in% colnames(data_dt)
  )

  id_leftover <- data_dt[, get(id_q)]

  if (!is.null(top_word_freq)) {

  word_freq <- unnest_tokens(data_dt, output = word, input = value, token = "words")

  word_freq <- word_freq[str_length(word) < 19 & str_detect(word, "^[a-zA-Z']+$") & !str_detect(word, "([a-zA-Z]+)\\1{3,}"),
                          .N, by = .(word)][
                          order(N, decreasing = TRUE)]

  word_freq[, freq := N / sum(N)][, cum_freq := cumsum(freq)]

  freq_words <- word_freq[cum_freq < top_word_freq, word]

  }

  for (i in 1:(n - 1)) {
    ngram_init <- data_dt[get(id_q) %in% id_leftover]

    ngram_init <- unnest_tokens(ngram_init, output = ngram, input = value_q, token = "ngrams", n = n + 1 - i)

    # ngram_init <- unnest_tokens(data_dt, output = ngram, input = value_q, token = "ngrams", n = n)

    split <- quote(`:=`(
      eval(word_vec[i:n]),
      # eval(word_vec[1:n]),
      tstrsplit(
        eval(ngram),
        split = " ",
        fill = NA,
        fixed = TRUE
      )
    ))

    ngram_c <- copy(ngram_init)

    ngram_c[, eval(split)][, order := n + 1 - i]
    # ngram_c[, eval(split)][, order := n]

    ngram_final <- rbind(ngram_final, ngram_c[!is.na(ngram)], use.names = TRUE, fill = TRUE)
    # ngram_final <- ngram_c[!is.na(ngram)]


    id_leftover <- ngram_c[is.na(ngram), get(id_q)]

  }

  for (i in word_vec) {
    ngram_final <- ngram_final[str_length(get(i)) < (word_len_max + 1) &
      str_detect(get(i), "^[a-zA-Z']+$") &
      !str_detect(get(i), "([a-zA-Z]+)\\1{3,}") | is.na(get(i))]

    if (!is.null(profanity_filter)) {

      # profanity <- unlist(profanity_filter)

      ngram_final <- ngram_final[!(get(i) %in% profanity_filter)]

    }

    if (!is.null(top_word_freq)) {

      ngram_final <- ngram_final[get(i) %in% freq_words]

    }

  }

  ngram_final <- ngram_final[, .N, by = c(word_vec, "order")]

  ngram_final
}

```

```{r kneser-ney}

kn_smoothing <- function(data, discount_factor = .75) {

  dt <- copy(as.data.table(data))
  # dt <- copy(as.data.table(ngram))
  n <- length(dt) - 2
  word_vec <- paste0("word_", 1:n)
  id_vec <- paste0("id_", 1:n)
  prob_vec <- paste0("prob_", 1:n)
  keep_cols <- c(id_vec, prob_vec)
  
  ngram_list <- map(1:n, ~ dt[!is.na(get(word_vec[.x])), .(count = sum(N)), by = c(word_vec[.x:n])])
  list_name <- paste0("order_", n:1)
  names(ngram_list) <- list_name
  ccount_nom <- paste0("ccount_nom_", list_name[2:n])
  ccount_denom <- paste0("ccount_denom_", list_name[2:n])
  ccount <- paste0("ccount_", list_name[2:n])
  lambda <- paste0("lambda_", list_name[1:n])
  pcont <- paste0("pcont_", list_name[1:n])

  # top ngram level

  ngram_list[[list_name[1]]][, probability := count / sum(count), by = c(word_vec[1:(n-1)])]

  # Recursive continuation count

  for (i in 1:(n-2)) {

    temp_1 <- copy(ngram_list[[list_name[i]]])
    # temp_1 <- ngram_list[[list_name[i]]][, (ccount_nom[i]) := .N, by = c(word_vec[(i + 1):n])]
    temp_1[, (ccount_nom[i]) := .N, by = c(word_vec[(i + 1):n])]

    temp_2 <- unique(temp_1[, .SD, .SDcols = c(word_vec[i:(n-1)])])
    temp_2 <- temp_2[, (ccount_denom[i]) := .N, by = c(word_vec[(i + 1):(n-1)])][
                     , .SD, .SDcols = c(word_vec[(i + 1):(n-1)], ccount_denom[i])]
    temp_2 <- unique(temp_2)

    temp <- merge(temp_1, temp_2, by = c(word_vec[(i + 1):(n-1)]), all.x = TRUE)
    temp[, (ccount[i]) := (get(ccount_nom[i]) - discount_factor) / get(ccount_denom[i])]
    temp <- unique(temp[, .SD, .SDcols = c(word_vec[(i + 1):n], ccount_nom[i], ccount_denom[i], ccount[i])])

    ngram_list[[list_name[(i + 1)]]] <- merge(ngram_list[[list_name[(i + 1)]]],
                                              temp,
                                              by = c(word_vec[(i + 1):n]),
                                              all.x = TRUE,
                                              all.y = FALSE)
  }

  # Recursive calculations

  for (i in 2:(n-1)) {

    # Continuation Probability

    pcont_denom <- nrow(ngram_list[[list_name[i]]])
    pcont_temp <- unique(ngram_list[[list_name[i]]][, .SD, .SDcols = word_vec[i:n]])[
                                                    , .N, by = c(word_vec[n])][
                                                    , (pcont[i]) := N / as.numeric(pcont_denom)]

    ngram_list[[list_name[i]]] <- merge(ngram_list[[list_name[i]]], pcont_temp, by = c(word_vec[n]), all.x = TRUE)

    # Lambda

    lambda_temp <- ngram_list[[list_name[i]]][, .SD, .SDcols = c(word_vec[i:n], "count")]

    lambda_temp[, `:=`(lambda_nom = .N, lambda_denom = sum(count)), by = c(word_vec[i:(n-1)])][
                , (lambda[i]) := discount_factor * lambda_nom / lambda_denom]

    lambda_temp <- unique(lambda_temp[, .SD, .SDcols = c(word_vec[i:(n-1)], lambda[i])])

    ngram_list[[list_name[i]]] <- merge(ngram_list[[list_name[i]]], lambda_temp, by = c(word_vec[i:(n-1)]), all.x = TRUE)

    ngram_list[[list_name[i]]][, probability := get(ccount[(i - 1)]) + get(lambda[i]) * get(pcont[i])]

  }

  ngram_list[[list_name[n]]][, probability := count / sum(count)]

  for (i in 1:n) {

    ngram_list[[list_name[i]]] <- ngram_list[[list_name[i]]][, .SD, .SDcols = c(word_vec[i:n], "probability")]
    setnames(ngram_list[[list_name[i]]], old = "probability", new = prob_vec[i])
    
    if (i == 1) {
      
      table <- ngram_list[[list_name[1]]]
      
    } else {
      
      table <- merge(table, ngram_list[[list_name[i]]], by = word_vec[i:n])
      
    }
    
  }
  
  words <- as.data.table(unique(unlist(ngram[, ..word_vec])))
  setnames(words, old = "V1", new ="word")
  words[, id := .I]
  
  for (i in 1:n) {
    
    table <- merge(table, words, by.x = word_vec[i], by.y = "word")

    setnames(table, old = "id", new = id_vec[i])
    
  }
  
  table <- table[, .SD, .SDcols = keep_cols]
  
  l <- list(lookup = words, ngram_table = table)
  
  l
  
}

```


```{r}

# train_ngram <- make_ngram(train, n = 4, word_len_max = 18, top_word_freq = NULL, profanity_filter = profanity, id = id, value = value)

ngram <- make_ngram(corpora, n = 6, word_len_max = 18, top_word_freq = NULL, profanity_filter = profanity, id = id, value = text)
ngram_l <- kn_smoothing(ngram, .75)

lookup <- ngram_l[["lookup"]]
table <- ngram_l[["ngram_table"]]

# train_ngram <- make_ngram(train, n = 6, word_len_max = 18, top_word_freq = NULL, profanity_filter = profanity, id = id, value = text)
# test_ngram <- make_ngram(test, n = 6, word_len_max = 18, top_word_freq = NULL, profanity_filter = profanity, id = id, value = text)
# 
# train_kn <- kn_smoothing(train_ngram, .75)

```

```{r predict}

predict_kn <- function(sentence, ngram_lookup, ngram_table = table, n_words_predict = 3, profanity_filter = profanity, word_len_max = 18) {
  
  if (sentence == "" | is.na(sentence) | is.null(sentence)) {
    
  out_string <- c("the", "on", "a") 
  
  } else {

  c <- ncol(ngram_table) / 2 - 1
  
  # sentence <- "Blog Search is a resource site which provides a collection of links to the best blogs available on the" # Web
  # c <- 5
  
  # Apply pre-processing to sentence
  
  char <- strsplit(sentence, split = " ")
  names(char) <- "char"
  sen <- as.data.table(char)
  
  sen <- as.data.table(strsplit(sentence, split = " "))
  
  sen <- sen[str_detect(char, "^[a-zA-Z']+$") & 
             !str_detect(char, "([a-zA-Z]+)\\1{3,}") & 
             str_length(char) < (word_len_max + 1), .(char = str_to_lower(char))]
  
  if (!is.null(profanity_filter)) { sen <- sen[!(char %in% profanity_filter)] }
  
  sen <- tail(sen, c)

  sen <- unlist(merge(sen, ngram_lookup, by.x = "char", by.y = "word")[, .(id)])
  
  id_vec <- paste0("id_", 1:(c + 1))

  string <- character(0)
  string <- sapply(1:c, \(x) paste0("(", id_vec[x], " == ", sen[x], ")"))

  # string <- paste0(string, collapse = " & ")

  s <- list(NULL)
  dat <- list(NULL)
  n <- 0
  # k <- 0
  for (i in 1:c) {
    
    if (n < n_words_predict) {
      
      j <- n_words_predict - n
      
      s[[i]] <- paste0(string[i:c], collapse = " & ")
      
      temp <- unique(ngram_table[eval(str2expression(s[[i]])), .SD, .SDcols = c(id_vec[i:c + 1], paste0("prob_", i))])
        
      temp <- setorderv(temp, cols = paste0("prob_", i), order = -1)
      temp <- head(temp, j)
      
      dat[[i]] <- temp
      
      setnames(dat[[i]], old = paste0("prob_", i), new = "prob")
    
      n <- n + nrow(dat[[i]])
      k <- i
    }
    
  }
  
  out <- unique(rbindlist(dat, fill = TRUE))
  
  out <- merge(out, ngram_lookup, by.x = paste0("id_", c + 1), by.y = "id", sort = FALSE)
  
  out_string <- out[, word]
  
  }
  
  out_string
  
}

```

