---
title: "Model_example"
author: "Konstantinos Patelis"
date: "24/04/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r libraries}

library(here)
library(tidyverse)
library(data.table)
library(dtplyr)
library(tidyfast)
library(tidytext)
library(lexicon)
library(knitr)
theme_set(theme_bw())

source(here("analysis", "functions.R"))
```


```{r load_data, warning=FALSE}

tweets_loc <- here("data", "final", "en_US", "en_US.twitter.txt")
news_loc <- here("data", "final", "en_US", "en_US.news.txt")
blogs_loc <- here("data", "final", "en_US", "en_US.blogs.txt")

tweets <- read_text(tweets_loc, prows = 1, seed = 1234) %>%
  as_tibble() %>%
  mutate(source = "tweets", id = paste0("tweet_", row_number()))

news <- read_text(news_loc, prows = 1, seed = 5678) %>%
  as_tibble() %>%
  mutate(source = "news", id = paste0("news_", row_number()))

blogs <- read_text(blogs_loc, prows = 1, seed = 9012) %>%
  as_tibble() %>%
  mutate(source = "blogs", id = paste0("blog_", row_number()))

corpora <- bind_rows(tweets, news, blogs)

profanity <- tibble(word = c(
  profanity_alvarez,
  profanity_arr_bad,
  profanity_banned,
  profanity_racist,
  profanity_zac_anger
)) %>%
  distinct()

text_split <- split_text(tweets_loc, news_loc, blogs_loc, names = c("tweets", "news", "blogs"), split = .8, seed = 1234)

train <- text_split$train
test <- text_split$test

```

```{r}

make_ngram <- function(data, n = 4, id = id, value = value, word_len_max = 18, top_word_freq = NULL, profanity_filter = NULL) {
  id_q <- deparse(substitute(id))
  value_q <- deparse(substitute(value))
  word_vec <- paste0("word_", 1:n)
  ngram_final <- NULL
  data_dt <- copy(as.data.table(data))

  stopifnot(
    "Please specify existing id name" = id_q %in% colnames(data_dt),
    "Please specify existing text column name" = value_q %in% colnames(data_dt)
  )

  id_leftover <- data_dt[, get(id_q)]

  if (!is.null(top_word_freq)) {

  word_freq <- unnest_tokens(data_dt, output = word, input = value, token = "words")

  word_freq <- word_freq[str_length(word) < 19 & str_detect(word, "^[a-zA-Z']+$") & !str_detect(word, "([a-zA-Z]+)\\1{3,}"),
                          .N, by = .(word)][
                          order(N, decreasing = TRUE)]

  word_freq[, freq := N / sum(N)][, cum_freq := cumsum(freq)]

  freq_words <- word_freq[cum_freq < top_word_freq, word]

  }

  for (i in 1:(n - 1)) {
    ngram_init <- data_dt[get(id_q) %in% id_leftover]

    ngram_init <- unnest_tokens(ngram_init, output = ngram, input = value_q, token = "ngrams", n = n + 1 - i)

    # ngram_init <- unnest_tokens(data_dt, output = ngram, input = value_q, token = "ngrams", n = n)

    split <- quote(`:=`(
      eval(word_vec[i:n]),
      # eval(word_vec[1:n]),
      tstrsplit(
        eval(ngram),
        split = " ",
        fill = NA,
        fixed = TRUE
      )
    ))

    ngram_c <- copy(ngram_init)

    ngram_c[, eval(split)][, order := n + 1 - i]
    # ngram_c[, eval(split)][, order := n]

    ngram_final <- rbind(ngram_final, ngram_c[!is.na(ngram)], use.names = TRUE, fill = TRUE)
    # ngram_final <- ngram_c[!is.na(ngram)]


    id_leftover <- ngram_c[is.na(ngram), get(id_q)]

  }

  for (i in word_vec) {
    ngram_final <- ngram_final[str_length(get(i)) < (word_len_max + 1) &
      str_detect(get(i), "^[a-zA-Z']+$") &
      !str_detect(get(i), "([a-zA-Z]+)\\1{3,}") | is.na(get(i))]

    if (!is.null(profanity_filter)) {

      profanity <- unlist(profanity_filter)

      ngram_final <- ngram_final[!(get(i) %in% profanity)]

    }

    if (!is.null(top_word_freq)) {

      ngram_final <- ngram_final[get(i) %in% freq_words]

    }

  }

  ngram_final <- ngram_final[, .N, by = c(word_vec, "order")]

  ngram_final
}

```

```{r}

ngram <- make_ngram(corpora, n = 4, word_len_max = 18, top_word_freq = NULL, profanity_filter = profanity, id = id, value = value)

```

```{r kneser-ney}

# kn_smoothing <- function(data, discount_factor = .75) {
#   
#   dt <- copy(as.data.table(data))
#   # dt <- copy(as.data.table(ngram))
#   n <- length(dt) - 2
#   word_vec <- paste0("word_", 1:n)
#   
#   ngram_list <- map(1:n, ~ dt[!is.na(get(word_vec[.x])), .(count = sum(N)), by = c(word_vec[.x:n])])
#   list_name <- paste0("order_", n:1)
#   names(ngram_list) <- list_name
#   ccount_nom <- paste0("ccount_nom_", list_name[2:n])
#   ccount_denom <- paste0("ccount_denom_", list_name[2:n])
#   ccount <- paste0("ccount_", list_name[2:n])
#   lambda <- paste0("lambda_", list_name[1:n])
#   pcont <- paste0("pcont_", list_name[1:n])
#   
#   # top ngram level
#   
#   ngram_list[[list_name[1]]][, probability := count / sum(count), by = c(word_vec[1:(n-1)])]
#   
#   # Recursive continuation count
#   
#   for (i in 1:(n-2)) {
#     
#     temp_1 <- copy(ngram_list[[list_name[i]]])
#     # temp_1 <- ngram_list[[list_name[i]]][, (ccount_nom[i]) := .N, by = c(word_vec[(i + 1):n])]
#     temp_1[, (ccount_nom[i]) := .N, by = c(word_vec[(i + 1):n])]
#     
#     temp_2 <- unique(temp_1[, .SD, .SDcols = c(word_vec[i:(n-1)])])
#     temp_2 <- temp_2[, (ccount_denom[i]) := .N, by = c(word_vec[(i + 1):(n-1)])][
#                      , .SD, .SDcols = c(word_vec[(i + 1):(n-1)], ccount_denom[i])]
#     temp_2 <- unique(temp_2)
#   
#     temp <- merge(temp_1, temp_2, by = c(word_vec[(i + 1):(n-1)]), all.x = TRUE)
#     temp[, (ccount[i]) := (get(ccount_nom[i]) - discount_factor) / get(ccount_denom[i])]
#     temp <- unique(temp[, .SD, .SDcols = c(word_vec[(i + 1):n], ccount_nom[i], ccount_denom[i], ccount[i])])
#   
#     ngram_list[[list_name[(i + 1)]]] <- merge(ngram_list[[list_name[(i + 1)]]], 
#                                               temp, 
#                                               by = c(word_vec[(i + 1):n]),
#                                               all.x = TRUE, 
#                                               all.y = FALSE)
#   }
#   
#   # temp_1 <- ngram_list[[list_name[1]]][, (ccount_nom[1]) := .N, by = c(word_vec[2:n])]
#   # 
#   # temp_2 <- unique(temp_1[, .SD, .SDcols = c(word_vec[1:(n-1)])])
#   # temp_2 <- temp_2[, (ccount_denom[1]) := .N, by = c(word_vec[2:(n-1)])][, .SD, .SDcols = c(word_vec[2:(n-1)], ccount_denom[1])]
#   # temp_2 <- unique(temp_2)
#   # 
#   # temp <- merge(temp_1, temp_2, by = c(word_vec[2:(n-1)]), all.x = TRUE)
#   # temp[, (ccount[1]) := (get(ccount_nom[1]) - .75) / get(ccount_denom[1])]
#   # temp <- unique(temp[, .SD, .SDcols = c(word_vec[2:n], ccount_nom[1], ccount_denom[1], ccount[1])])
#   # 
#   # ngram_list[[list_name[2]]] <- merge(ngram_list[[list_name[2]]], 
#   #                                     temp, 
#   #                                     by = c(word_vec[2:n]),
#   #                                     all.x = TRUE, 
#   #                                     all.y = FALSE)
#   
#   # Recursive calculations
#   
#   for (i in 2:(n-1)) {
#     
#     # Continuation Probability
#     
#     pcont_denom <- nrow(ngram_list[[list_name[i]]])
#     pcont_temp <- unique(ngram_list[[list_name[i]]][, .SD, .SDcols = word_vec[i:n]])[
#                                                     , .N, by = c(word_vec[n])][
#                                                     , (pcont[i]) := N / as.numeric(pcont_denom)]
#     
#     ngram_list[[list_name[i]]] <- merge(ngram_list[[list_name[i]]], pcont_temp, by = c(word_vec[n]), all.x = TRUE)
#     
#     # Lambda
#     
#     lambda_temp <- ngram_list[[list_name[i]]][, .SD, .SDcols = c(word_vec[i:n], "count")]
#   
#     lambda_temp[, `:=`(lambda_nom = .N, lambda_denom = sum(count)), by = c(word_vec[i:(n-1)])][
#                 , (lambda[i]) := discount_factor * lambda_nom / lambda_denom]
#   
#     lambda_temp <- unique(lambda_temp[, .SD, .SDcols = c(word_vec[i:(n-1)], lambda[i])])
#   
#     ngram_list[[list_name[i]]] <- merge(ngram_list[[list_name[i]]], lambda_temp, by = c(word_vec[i:(n-1)]), all.x = TRUE)
#     
#     ngram_list[[list_name[i]]][, probability := get(ccount[(i - 1)]) + get(lambda[i]) * get(pcont[i])]
#     
#   }
#   
#   ngram_list[[list_name[n]]][, probability := count / sum(count)] 
#   
#   for (i in 1:n) {
#     
#     ngram_list[[list_name[i]]] <- ngram_list[[list_name[i]]][, .SD, .SDcols = c(word_vec[i:n], "probability")]
#     
#   }
#   
#   dt <- do.call(rbind, args = c(ngram_list, fill = TRUE))
#   
#   # ngram_list
#   dt
#   
# }

```

```{r}

library(tictoc)

tic()

kn <- kn_smoothing(ngram, discount_factor = .75)

toc()

```

